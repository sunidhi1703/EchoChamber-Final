# ğŸ—£ï¸ Echo Chamber: Conversational AI Fine-Tuning Project

This project is about fine-tuning a conversational AI model using movie dialogues.

## ğŸ“Œ Overview

I am using **DialoGPT** (a transformer-based conversational model from Hugging Face) as our base model. It is already pretrained on a large Reddit conversation dataset.

I fine-tuned it further using the **Cornell Movie Dialogues Corpus**, which contains over 220,000 lines of movie conversations.

---

## ğŸ“ Project Structure

EchoChamber-SoC/
â”‚
â”œâ”€â”€ dialogpt-finetuned/
â”‚   â””â”€â”€ final/                    # Fine-tuned model 
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ vocab.json
â”‚       â”œâ”€â”€ merges.txt
â”‚       â””â”€â”€ ...                  # Other model/tokenizer files
â”‚
â”œâ”€â”€ .gitattributes               # Git LFS tracking config for large model files
â”œâ”€â”€ app.py                       # Main Python app (Streamlit)
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    


## ğŸ“Š Dataset: Cornell Movie Dialogues Corpus

- `movie_lines.txt`
- `movie_conversations.txt` 

In this i have used **10,000 dialogue pairs** to train the model.

---

## ğŸ› ï¸ Changes Made

### 1. ğŸ”„ Increased Dataset Size

### 2. ğŸ” Increased Epochs
Reduced loss from 0.7 to 0.5
### 3. ğŸ§  Tokenization Update
- Added `"human:"` and `"bot:"` labels to each training sample.
```python
formatted_text = f"human: {prompt}\nbot: {response}"
